{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import os\n",
    "def mk_dir(file_path):\n",
    "\n",
    "    folder = os.path.exists(file_path)\n",
    "    if not folder:\n",
    "        os.makedirs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"Virology\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文数字转换程序\n",
    "import re\n",
    "_known = {\n",
    "  'zero': 0,\n",
    "  'one': 1,\n",
    "  'a':1,\n",
    "  'an':1,\n",
    "  'case':1,\n",
    "  'two': 2,\n",
    "  'both':2,\n",
    "  'three': 3,\n",
    "  'four': 4,\n",
    "  'five': 5,\n",
    "  'six': 6,\n",
    "  'seven': 7,\n",
    "  'eight': 8,\n",
    "  'nine': 9,\n",
    "  'ten': 10,\n",
    "  'eleven': 11,\n",
    "  'twelve': 12,\n",
    "  'thirteen': 13,\n",
    "  'fourteen': 14,\n",
    "  'fifteen': 15,\n",
    "  'sixteen': 16,\n",
    "  'seventeen': 17,\n",
    "  'eighteen': 18,\n",
    "  'nineteen': 19,\n",
    "  'twenty': 20,\n",
    "  'thirty': 30,\n",
    "  'forty': 40,\n",
    "  'fifty': 50,\n",
    "  'sixty': 60,\n",
    "  'seventy': 70,\n",
    "  'eighty': 80,\n",
    "  'ninety': 90\n",
    "  }\n",
    "def spoken_word_to_number(n):\n",
    "  \"\"\"Assume n is a positive integer\".\n",
    "assert _positive_integer_number('nine hundred') == 900\n",
    "assert spoken_word_to_number('one hundred') == 100\n",
    "assert spoken_word_to_number('eleven') == 11\n",
    "assert spoken_word_to_number('twenty two') == 22\n",
    "assert spoken_word_to_number('thirty-two') == 32\n",
    "assert spoken_word_to_number('forty two') == 42\n",
    "assert spoken_word_to_number('two hundred thirty two') == 232\n",
    "assert spoken_word_to_number('two thirty two') == 232\n",
    "assert spoken_word_to_number('nineteen hundred eighty nine') == 1989\n",
    "assert spoken_word_to_number('nineteen eighty nine') == 1989\n",
    "assert spoken_word_to_number('one thousand nine hundred and eighty nine') == 1989\n",
    "assert spoken_word_to_number('nine eighty') == 980\n",
    "assert spoken_word_to_number('nine two') == 92 # wont be able to convert this one\n",
    "assert spoken_word_to_number('nine thousand nine hundred') == 9900\n",
    "assert spoken_word_to_number('one thousand nine hundred one') == 1901\n",
    "\"\"\"\n",
    "  n = n.lower().strip()\n",
    "  if n.strip() not in _known:\n",
    "    output = \"Not\"\n",
    "  else:\n",
    "    if n in _known:\n",
    "      return _known[n]\n",
    "    else:\n",
    "      inputWordArr = re.split('[ -]', n)\n",
    "    assert len(inputWordArr) > 1 #all single words are known\n",
    "    #Check the pathological case where hundred is at the end or thousand is at end\n",
    "    if inputWordArr[-1] == 'hundred':\n",
    "      inputWordArr.append('zero')\n",
    "      inputWordArr.append('zero')\n",
    "    if inputWordArr[-1] == 'thousand':\n",
    "      inputWordArr.append('zero')\n",
    "      inputWordArr.append('zero')\n",
    "      inputWordArr.append('zero')\n",
    "    if inputWordArr[0] == 'hundred':\n",
    "      inputWordArr.insert(0, 'one')\n",
    "    if inputWordArr[0] == 'thousand':\n",
    "      inputWordArr.insert(0, 'one')\n",
    "    inputWordArr = [word for word in inputWordArr if word not in ['and', 'minus', 'negative']]\n",
    "    currentPosition = 'unit'\n",
    "    prevPosition = None\n",
    "    output = 0\n",
    "    for word in reversed(inputWordArr):\n",
    "      if currentPosition == 'unit':\n",
    "        number = _known[word]\n",
    "        output += number\n",
    "        if number > 9:\n",
    "          currentPosition = 'hundred'\n",
    "        else:\n",
    "          currentPosition = 'ten'\n",
    "      elif currentPosition == 'ten':\n",
    "        if word != 'hundred':\n",
    "          number = _known[word]\n",
    "          if number < 10:\n",
    "            output += number*10\n",
    "          else:\n",
    "            output += number\n",
    "        #else: nothing special\n",
    "        currentPosition = 'hundred'\n",
    "      elif currentPosition == 'hundred':\n",
    "        if word not in [ 'hundred', 'thousand']:\n",
    "          number = _known[word]\n",
    "          output += number*100\n",
    "          currentPosition = 'thousand'\n",
    "        elif word == 'thousand':\n",
    "          currentPosition = 'thousand'\n",
    "        else:\n",
    "          currentPosition = 'hundred'\n",
    "      elif currentPosition == 'thousand':\n",
    "        assert word != 'hundred'\n",
    "        if word != 'thousand':\n",
    "          number = _known[word]\n",
    "          output += number*1000\n",
    "      else:\n",
    "        assert \"Can't be here\" == None\n",
    "  return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ab = \"num\"\n",
    "evi_yn = json.load(open(\"../6.Bert/output/output_%s/%s/nbest_predictions_.json\" % (col_ab,file_dir), \"r\"))\n",
    "pro_l = {}\n",
    "txt_l = {}\n",
    "for i in evi_yn:\n",
    "    pro = [j[\"probability\"] for j in evi_yn[i]]\n",
    "    pro_l[i] = np.max(pro)\n",
    "pro_df = pd.DataFrame(pro_l, index=[\"rate\"]).T\n",
    "pro_df = pro_df.loc[pro_df[\"rate\"] > 0.1,:]\n",
    "evi = pd.DataFrame(json.load(open(\"../6.Bert/output/output_%s/%s/predictions_.json\" % (col_ab,file_dir), \"r\")), \\\n",
    "    index=[col_ab]).T\n",
    "evi_df = evi.loc[pro_df.index]\n",
    "# 数字转换\n",
    "evi_df[\"%s_origin\" % col_ab] = evi_df[col_ab]\n",
    "\n",
    "for k in evi_df.index:\n",
    "        \n",
    "    evi_str = str.lower(evi_df.loc[k, col_ab].strip())\n",
    "    evi_split = re.split(r'\\W', evi_str)\n",
    "    evi_split = list(set(evi_split)) # 去重\n",
    "    \n",
    "    for dig in evi_split:\n",
    "        \n",
    "        if dig.isdigit():\n",
    "            evi_df.loc[k, col_ab] = dig\n",
    "        else:\n",
    "            if type(spoken_word_to_number(dig)) is int:\n",
    "                evi_df.loc[k, col_ab] = str(spoken_word_to_number(dig))\n",
    "\n",
    "    if evi_df.loc[k, col_ab].isdigit():\n",
    "        if int(evi_df.loc[k, col_ab]) == 1:\n",
    "                evi_df.loc[k, col_ab] == \"1\"\n",
    "        else:\n",
    "            if int(evi_df.loc[k, col_ab]) < 10:\n",
    "                evi_df.loc[k, col_ab] = \"1-10\"\n",
    "            else:\n",
    "                if int(evi_df.loc[k, col_ab]) < 100:\n",
    "                    evi_df.loc[k, col_ab] = \"10-100\"\n",
    "                else:\n",
    "                    evi_df.loc[k, col_ab] = \">100\"\n",
    "    if evi_df.loc[k, col_ab] not in [\"1\", \"1-10\", \"10-100\", \">100\"]:\n",
    "        # print(evi_df.loc[k, col_ab], evi_str)\n",
    "        evi_df.loc[k, col_ab] = \"none\"\n",
    "\n",
    "evi_df.to_csv(\"output/%s/%s_df.csv\" % (file_dir,col_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10575"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evi_df[\"num\"] == \"none\")/evi_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 年龄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文数字转换程序\n",
    "import re\n",
    "_known = {\n",
    "  'zero': 0,\n",
    "  'two': 2,\n",
    "  'three': 3,\n",
    "  'four': 4,\n",
    "  'five': 5,\n",
    "  'six': 6,\n",
    "  'seven': 7,\n",
    "  'eight': 8,\n",
    "  'nine': 9,\n",
    "  'ten': 10,\n",
    "  'eleven': 11,\n",
    "  'twelve': 12,\n",
    "  'thirteen': 13,\n",
    "  'fourteen': 14,\n",
    "  'fifteen': 15,\n",
    "  'sixteen': 16,\n",
    "  'seventeen': 17,\n",
    "  'eighteen': 18,\n",
    "  'nineteen': 19,\n",
    "  'twenty': 20,\n",
    "  'thirty': 30,\n",
    "  'forty': 40,\n",
    "  'fifty': 50,\n",
    "  'sixty': 60,\n",
    "  'seventy': 70,\n",
    "  'eighty': 80,\n",
    "  'ninety': 90\n",
    "  }\n",
    "def spoken_word_to_number(n):\n",
    "  \"\"\"Assume n is a positive integer\".\n",
    "assert _positive_integer_number('nine hundred') == 900\n",
    "assert spoken_word_to_number('one hundred') == 100\n",
    "assert spoken_word_to_number('eleven') == 11\n",
    "assert spoken_word_to_number('twenty two') == 22\n",
    "assert spoken_word_to_number('thirty-two') == 32\n",
    "assert spoken_word_to_number('forty two') == 42\n",
    "assert spoken_word_to_number('two hundred thirty two') == 232\n",
    "assert spoken_word_to_number('two thirty two') == 232\n",
    "assert spoken_word_to_number('nineteen hundred eighty nine') == 1989\n",
    "assert spoken_word_to_number('nineteen eighty nine') == 1989\n",
    "assert spoken_word_to_number('one thousand nine hundred and eighty nine') == 1989\n",
    "assert spoken_word_to_number('nine eighty') == 980\n",
    "assert spoken_word_to_number('nine two') == 92 # wont be able to convert this one\n",
    "assert spoken_word_to_number('nine thousand nine hundred') == 9900\n",
    "assert spoken_word_to_number('one thousand nine hundred one') == 1901\n",
    "\"\"\"\n",
    "  n = n.lower().strip()\n",
    "  if n.strip() not in _known:\n",
    "    output = \"Not\"\n",
    "  else:\n",
    "    if n in _known:\n",
    "      return _known[n]\n",
    "    else:\n",
    "      inputWordArr = re.split('[ -]', n)\n",
    "    assert len(inputWordArr) > 1 #all single words are known\n",
    "    #Check the pathological case where hundred is at the end or thousand is at end\n",
    "    if inputWordArr[-1] == 'hundred':\n",
    "      inputWordArr.append('zero')\n",
    "      inputWordArr.append('zero')\n",
    "    if inputWordArr[-1] == 'thousand':\n",
    "      inputWordArr.append('zero')\n",
    "      inputWordArr.append('zero')\n",
    "      inputWordArr.append('zero')\n",
    "    if inputWordArr[0] == 'hundred':\n",
    "      inputWordArr.insert(0, 'one')\n",
    "    if inputWordArr[0] == 'thousand':\n",
    "      inputWordArr.insert(0, 'one')\n",
    "    inputWordArr = [word for word in inputWordArr if word not in ['and', 'minus', 'negative']]\n",
    "    currentPosition = 'unit'\n",
    "    prevPosition = None\n",
    "    output = 0\n",
    "    for word in reversed(inputWordArr):\n",
    "      if currentPosition == 'unit':\n",
    "        number = _known[word]\n",
    "        output += number\n",
    "        if number > 9:\n",
    "          currentPosition = 'hundred'\n",
    "        else:\n",
    "          currentPosition = 'ten'\n",
    "      elif currentPosition == 'ten':\n",
    "        if word != 'hundred':\n",
    "          number = _known[word]\n",
    "          if number < 10:\n",
    "            output += number*10\n",
    "          else:\n",
    "            output += number\n",
    "        #else: nothing special\n",
    "        currentPosition = 'hundred'\n",
    "      elif currentPosition == 'hundred':\n",
    "        if word not in [ 'hundred', 'thousand']:\n",
    "          number = _known[word]\n",
    "          output += number*100\n",
    "          currentPosition = 'thousand'\n",
    "        elif word == 'thousand':\n",
    "          currentPosition = 'thousand'\n",
    "        else:\n",
    "          currentPosition = 'hundred'\n",
    "      elif currentPosition == 'thousand':\n",
    "        assert word != 'hundred'\n",
    "        if word != 'thousand':\n",
    "          number = _known[word]\n",
    "          output += number*1000\n",
    "      else:\n",
    "        assert \"Can't be here\" == None\n",
    "  return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ab = \"age\"\n",
    "evi_yn = json.load(open(\"../6.Bert/output/output_%s/%s/nbest_predictions_.json\" % (col_ab, file_dir), \"r\"))\n",
    "pro_l = {}\n",
    "txt_l = {}\n",
    "for i in evi_yn:\n",
    "    pro = [j[\"probability\"] for j in evi_yn[i]]\n",
    "    pro_l[i] = np.max(pro)\n",
    "pro_df = pd.DataFrame(pro_l, index=[\"rate\"]).T\n",
    "pro_df = pro_df.loc[pro_df[\"rate\"] > 0.1,:]\n",
    "evi = pd.DataFrame(json.load(open(\"../6.Bert/output/output_%s/%s/predictions_.json\" % (col_ab,file_dir), \"r\")), \\\n",
    "    index=[col_ab]).T\n",
    "evi_df = evi.loc[pro_df.index]\n",
    "# 数字转换\n",
    "evi_df[\"%s_origin\" % col_ab] = evi_df[col_ab]\n",
    "\n",
    "for k in evi_df.index:\n",
    "        \n",
    "    evi_str = str.lower(evi_df.loc[k, col_ab].strip())\n",
    "    evi_split = re.split(r'\\W', evi_str)\n",
    "\n",
    "    for dig in evi_split:\n",
    "\n",
    "        if dig.split(\".\")[0].isdigit():\n",
    "            \n",
    "            if dig.split(\".\")[0].isdigit() < 100:\n",
    "\n",
    "                evi_df.loc[k, col_ab] = str(int(dig.split(\".\")[0]))\n",
    "             \n",
    "        else:\n",
    "\n",
    "            if type(spoken_word_to_number(dig)) is int:\n",
    "\n",
    "                if spoken_word_to_number(dig) < 100:\n",
    "\n",
    "                    evi_df.loc[k, col_ab] = str(spoken_word_to_number(dig))\n",
    "            else:\n",
    "                if dig in [\"infant\", \"infants\", \"neonatus\", \"neonate\",\"neonatal\",\n",
    "                \"day\", \"days\", \"month\", \"months\", \"months\"]:\n",
    "                    evi_df.loc[k, col_ab] = \"1岁以下\"\n",
    "                if dig in [\"young\", \"younger\", \"child\", \"children\",\"adolescent\", \"adolescents\", \"paediatric\", \"pediatric\"]:\n",
    "                    evi_df.loc[k, col_ab] = \"1-17岁\"\n",
    "                if dig in [\"adult\", \"adults\", \"middle\"]:\n",
    "                    evi_df.loc[k, col_ab] = \"17-65岁\"\n",
    "                if dig in [\"elder\", \"elderly\", \"older\", \"aging\"]:\n",
    "                    evi_df.loc[k, col_ab] = \">65岁\"\n",
    "        \n",
    "    if evi_df.loc[k, col_ab].isdigit():\n",
    "\n",
    "        if int(evi_df.loc[k, col_ab]) < 1:\n",
    "                evi_df.loc[k, col_ab] = \"1岁以下\"\n",
    "        else:\n",
    "            if int(evi_df.loc[k, col_ab]) < 17:\n",
    "                evi_df.loc[k, col_ab] = \"1-17岁\"\n",
    "            else:\n",
    "                if int(evi_df.loc[k, col_ab]) < 65:\n",
    "                    evi_df.loc[k, col_ab] = \"17-65岁\"\n",
    "                else:\n",
    "                    evi_df.loc[k, col_ab] = \">65岁\"\n",
    "        print(evi_df.loc[k, col_ab], evi_split)\n",
    "    \n",
    "    if evi_df.loc[k, col_ab] not in [\"1岁以下\", \"1-17岁\", \"17-65岁\", \">65岁\"]:\n",
    "        # print(evi_split)\n",
    "        evi_df.loc[k, col_ab] = \"none\"\n",
    "evi_df.to_csv(\"output/%s/%s_df.csv\" % (file_dir,col_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.315599173553719"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evi_df[\"age\"] == \"none\")/evi_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ab = \"gender\"\n",
    "evi_yn = json.load(open(\"../6.Bert/output/output_%s/%s/nbest_predictions_.json\" % (col_ab, file_dir), \"r\"))\n",
    "txt_l = {}\n",
    "for i in evi_yn:\n",
    "    txt0 = \" \"\n",
    "    txt = [j[\"text\"] for j in evi_yn[i]]\n",
    "    txt_l[i] = txt0.join(txt)\n",
    "pro_df = pd.DataFrame(txt_l, index=[col_ab]).T\n",
    "evi_df = pro_df\n",
    "\n",
    "# 添加标签\n",
    "evi_df[\"%s_origin\" % col_ab] = evi_df[col_ab]\n",
    "\n",
    "for k in evi_df.index:\n",
    "        \n",
    "    evi_str = str.lower(evi_df.loc[k, col_ab].strip())\n",
    "    evi_split = re.split(r'\\W', evi_str)\n",
    "    evi_split = list(set(evi_split)) # 去重\n",
    "\n",
    "    for dig in evi_split:\n",
    "        if dig in [\"man\", \"men\", \"boy\", \"boys\",\"male\", \"males\", \"he\", \"gentleman\", \"gentlemen\", \"his\", \"him\"]:\n",
    "            evi_df.loc[k, col_ab] = \"男\"\n",
    "        if dig in [\"woman\", \"women\", \"girl\",\"girls\", \"female\", \"females\", \"fm\", \"she\", \"lady\", \"ladys\", \"her\"]:\n",
    "            evi_df.loc[k, col_ab] = \"女\"\n",
    "    if evi_df.loc[k, col_ab] not in [\"男\",\"女\"]:\n",
    "        # print(evi_split)\n",
    "        evi_df.loc[k, col_ab] = \"none\"\n",
    "evi_df.to_csv(\"output/%s/%s_df.csv\" % (file_dir, col_ab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481120201384519"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evi_df[\"gender\"] == \"none\")/evi_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 免疫状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ab = \"immu\"\n",
    "evi_yn = json.load(open(\"../6.Bert/output/output_%s/%s/nbest_predictions_.json\" % (col_ab, file_dir), \"r\"))\n",
    "txt_l = {}\n",
    "for i in evi_yn:\n",
    "    txt0 = \" \"\n",
    "    txt = [j[\"text\"] for j in evi_yn[i]]\n",
    "    txt_l[i] = txt0.join(txt)\n",
    "pro_df = pd.DataFrame(txt_l, index=[col_ab]).T\n",
    "evi_df = pro_df\n",
    "\n",
    "# 添加标签\n",
    "sm_l = [\"immunodeficiency\", \"immunocompetent\", \"immunocompromised\", \"immunosuppressive\",\"immunosuppression\", \n",
    "        \"competent\", \"compromised\", \"suppressed\", \"supperssive\", \"suppression\",\n",
    "        \"hiv\",\"aids\", \"transplantation\", \"transplanted\"\n",
    "        \"malignancy\", \"cancer\", \"carcinoma\", \"leukemia\", \"lymphoma\",\n",
    "        \"ill\", \"impair\", \"impaired\",\"obesity\", \"diabete\", \"fibrosis\"]\n",
    "evi_df[\"%s_origin\" % col_ab] = evi_df[col_ab]\n",
    "\n",
    "for k in evi_df.index:\n",
    "        \n",
    "    evi_str = str.lower(evi_df.loc[k, col_ab].strip())\n",
    "    evi_split = re.split(r'\\W', evi_str)\n",
    "    evi_split = list(set(evi_split)) # 去重\n",
    "\n",
    "    for dig in evi_split:\n",
    "        if len(dig) >= 3:\n",
    "            for rm in sm_l:\n",
    "                if dig in rm:        \n",
    "                    evi_df.loc[k, col_ab] = \"抑制\"\n",
    "                if rm in dig:\n",
    "                    evi_df.loc[k, col_ab] = \"抑制\"\n",
    "    if evi_df.loc[k, col_ab] not in [\"抑制\"]:\n",
    "        # print(evi_split)\n",
    "        evi_df.loc[k, col_ab] = \"none\"\n",
    "evi_df.to_csv(\"output/%s/%s_df.csv\" % (file_dir,col_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5564820641913153"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evi_df[\"immu\"] == \"none\")/evi_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 证据等级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ab = \"evi\"\n",
    "evi_yn = json.load(open(\"../6.Bert/output/output_%s/%s/nbest_predictions_.json\" % (col_ab,file_dir), \"r\"))\n",
    "txt_l = {}\n",
    "for i in evi_yn:\n",
    "    txt0 = \" \"\n",
    "    txt = [j[\"text\"] for j in evi_yn[i]]\n",
    "    txt_l[i] = txt0.join(txt)\n",
    "pro_df = pd.DataFrame(txt_l, index=[col_ab]).T\n",
    "evi_df = pro_df\n",
    "# 添加标签\n",
    "\n",
    "evi_df[\"%s_origin\" % col_ab] = evi_df[col_ab]\n",
    "cla_l = [\"1：同时具备肺组织病理+病原学证据\",\n",
    "        \"2：肺组织病原学证据\",\n",
    "        \"3：血及无菌体液（如脓胸液）\",\n",
    "        \"4：气管镜活检组织培养出病原体,或者支气管肺泡灌洗液（BALF）定量培养>10**4\",\n",
    "        \"5：气管吸出物或痰/BALF纯培养或优势培养出病原体\",\n",
    "        \"6：气管吸出物或痰的分子诊断（PCR、基因组测序等）病原\",]\n",
    "for k in evi_df.index:\n",
    "        \n",
    "    evi_str = str.lower(evi_df.loc[k, col_ab].strip())\n",
    "    evi_split = re.split(r'\\W', evi_str)\n",
    "    evi_split = list(set(evi_split)) # 去重\n",
    "    for dig in evi_split:\n",
    "        if dig in [\"autopsy\", \"biopsy\", \"biopsies\", \"tissue\", \"tissues\", \"histopathology\", \"histological\", \"histopathological\"]:\n",
    "            evi_df.loc[k, col_ab] = \"1：同时具备肺组织病理+病原学证据\"\n",
    "        if dig in [\"pathogen\", \"pathology\", \"bacteriology\"]:\n",
    "            evi_df.loc[k, col_ab] = \"2：肺组织病原学证据\"\n",
    "        if dig in [\"blood\",\"bacteremia\", \"pleural\", \"purulent\"\n",
    "                    \"serology\", \"serological\", \"serum\", \"molecular\", \"igg\", \"igm\", \"antibodies\", \"antibody\"]:\n",
    "            evi_df.loc[k, col_ab] = \"3：血及无菌体液（如脓胸液）\"\n",
    "        if dig in [\"bronchoscopy\", \"tracheal\", \"specimen\", \"specimens\", \"bal\", \"bronchoalveolar\",\"lavage\"]:\n",
    "            evi_df.loc[k, col_ab] = \"4：气管镜活检组织培养出病原体,或者支气管肺泡灌洗液（BALF）定量培养>10**4\"\n",
    "        if dig in [\"aspirate\", \"aspirates\", \"aspiration\", \"secretions\", \"secretion\", \"sputum\"]:\n",
    "            evi_df.loc[k, col_ab] =\"5：气管吸出物或痰/BALF纯培养或优势培养出病原体\"\n",
    "        if dig in [\"pcr\",\"mpcr\",\"rtpcr\", \"polymerase\", \"16s\", \"elisa\", \"elisas\", \"nucleic\",\"biochemical\", \"immunofluorescence\",\"sequencing\", \n",
    "        \"sequence\",\"dna\", \"rna\", \"smears\", \"smears\",\"swabs\", \"swab\"]:\n",
    "            evi_df.loc[k, col_ab] = \"6：气管吸出物或痰的分子诊断（PCR、基因组测序等）病原\"\n",
    "    if evi_df.loc[k, col_ab] not in cla_l:\n",
    "        # print(k, evi_split)\n",
    "        evi_df.loc[k, col_ab] = \"none\"\n",
    "evi_df.to_csv(\"output/%s/%s_df.csv\" % (file_dir, col_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4052863436123348"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evi_df[\"evi\"] == \"none\")/evi_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并机器学习数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 人工标注的\n",
    "col_ab = \"pat\"\n",
    "evi = pd.DataFrame(json.load(open(\"../6.Bert/output/output_%s/%s/predictions_.json\" % (col_ab, file_dir), \"r\"))).T\n",
    "pat = evi.loc[evi[0]==\"yes\", :]\n",
    "for i in os.listdir(\"output/%s\" % file_dir):\n",
    "    df = pd.read_csv(\"output/%s/%s\" % (file_dir, i))\n",
    "    df.index = df.iloc[:, 0]\n",
    "    na_set =  list(set(pat.index) & set(df.index))\n",
    "\n",
    "    for j in na_set:\n",
    "        pat.loc[j, df.columns[1:3]] = df.loc[j, df.columns[1:3]]\n",
    "\n",
    "path = pat\n",
    "path[\"标注方式\"] = \"模型标注\"\n",
    "path_name  = [ele.split(\":\")[1].replace(\"\\n\", \"\") for ele in path.index]\n",
    "path[\"物种名称\"] = path_name\n",
    "link_l = [\"https://pubmed.ncbi.nlm.nih.gov/%s/\" % ele.split(\":\")[0].split(\"_\")[-1] for ele in path.index]\n",
    "path[\"原文链接\"] = link_l\n",
    "idx_l =  [ele.split(\":\")[0] for ele in path.index]\n",
    "path.index  = idx_l\n",
    "\n",
    "path.columns = ['是否是致病菌', '预测概率', '年龄', '年龄文本', '证据等级', '证据文本', '性别', '性别文本',\n",
    "               '免疫状态', '免疫文本', '样本量', '样本量文本', '标注方式', '物种名称', '原文链接']\n",
    "\n",
    "path = path[['物种名称', '是否是致病菌', '预测概率', '样本量', '样本量文本','年龄', '年龄文本',\n",
    "           '性别', '性别文本', '免疫状态', '免疫文本', '证据等级', '证据文本', '原文链接', '标注方式']]\n",
    "path.to_csv(\"output/%s/bert_label.csv\" % file_dir, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换为json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"Mycology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hum_df = pd.read_csv(\"human_label.csv\", header=0)\n",
    "md_df = pd.read_csv(\"output/%s/bert_label.csv\" % file_dir, header=0)\n",
    "# hum_df.columns = md_df.columns\n",
    "# mach_df = pd.concat((hum_df, md_df), 0)\n",
    "mach_df= deepcopy(md_df)\n",
    "mach_df[mach_df.isna()] = \"none\"\n",
    "mach_df[mach_df==\"NA\"] = \"none\"\n",
    "mach_df.index = range(mach_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加年份\n",
    "input_path = \"../2.PubTator/output/%s\" % file_dir\n",
    "\n",
    "# 建立物种大类列表6\n",
    "\n",
    "sp_l = [ele for ele in os.listdir(input_path) if \"_done\" not in ele]\n",
    "sp_l = [ele for ele in sp_l if \".ipynb\" not in ele]\n",
    "id_l = []\n",
    "year_l = []\n",
    "for sp in sp_l:\n",
    "\n",
    "    spec_l = [\n",
    "        ele for ele in os.listdir(\"%s/%s\" % (input_path, sp))\n",
    "        if \".ipynb\" not in ele\n",
    "    ]\n",
    "\n",
    "    for spec in spec_l:\n",
    "\n",
    "        file_l = [\n",
    "            ele for ele in os.listdir(\"%s/%s/%s\" % (input_path, sp, spec))\n",
    "            if \".json\" in ele\n",
    "        ]  # 建立这个物种的三种搜索结果json文件列表\n",
    "\n",
    "        for file in file_l:\n",
    "\n",
    "            # 读取json文件中的paper并建立列表\n",
    "            input_name = \"%s/%s/%s/%s\" % (input_path, sp, spec, file)\n",
    "            data_l = json.load(open(\"%s\" % input_name, 'r'))\n",
    "\n",
    "            for paper in data_l:\n",
    "\n",
    "                # 读取pubtator的注释\n",
    "                id = paper[\"id\"]\n",
    "                id_l.append(id)\n",
    "                year = paper[\"year\"]\n",
    "                year_l.append(year)\n",
    "a = pd.DataFrame((id_l, year_l))\n",
    "b = a.T.drop_duplicates()\n",
    "b.columns = [\"ID\", \"Year\"]\n",
    "mach_df[\"发表年份\"] = \"none\"\n",
    "\n",
    "for i in range(mach_df.shape[0]):\n",
    "\n",
    "    if mach_df[\"Unnamed: 0\"][i].split(\"_\")[-1] in id_l:\n",
    "\n",
    "        mach_df[\"发表年份\"][i]  = year_l[id_l.index(mach_df[\"Unnamed: 0\"][i].split(\"_\")[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "mach_dfy = deepcopy(mach_df)\n",
    "mach_dfy.loc[mach_dfy[\"样本量\"]==\"10-100\", \"样本量\"] =  \"11-100\"\n",
    "mach_dfy.loc[mach_dfy[\"样本量\"]==\"10-101\", \"样本量\"] =  \"11-100\"\n",
    "mach_dfy.loc[mach_dfy[\"样本量\"]==\"10-102\", \"样本量\"] =  \"11-100\"\n",
    "mach_dfy.loc[mach_dfy[\"样本量\"]==\"＞100\", \"样本量\"] =  \">100\"\n",
    "\n",
    "mach_dfy.loc[mach_dfy[\"年龄\"]==\"1-17\", \"年龄\"] =  \"1-17岁\"\n",
    "mach_dfy.loc[mach_dfy[\"年龄\"]==\"17-65\", \"年龄\"] =  \"18-65岁\"\n",
    "mach_dfy.loc[mach_dfy[\"年龄\"]==\"17-65岁\", \"年龄\"] =  \"18-65岁\"\n",
    "mach_dfy.loc[mach_dfy[\"年龄\"]==\"17-65岁\", \"年龄\"] =  \"18-65岁\"\n",
    "mach_dfy.loc[mach_dfy[\"年龄\"]==\">65岁\", \"年龄\"] =  \"65岁以上\"\n",
    "mach_dfy.loc[mach_dfy[\"年龄\"]==\"65以上\", \"年龄\"] =  \"65岁以上\"\n",
    "\n",
    "\n",
    "mach_dfy.loc[mach_dfy[\"免疫状态\"]==\"损伤\", \"免疫状态\"] =  \"抑制\"\n",
    "mach_dfy.loc[mach_dfy[\"免疫状态\"]==\"受损\", \"免疫状态\"] =  \"抑制\"\n",
    "\n",
    "mach_dfy.loc[mach_dfy[\"证据等级\"]==\"环境\", \"证据等级\"] =  \"none\"\n",
    "mach_dfy.loc[mach_dfy[\"证据等级\"]==\"8：文献报道\", \"证据等级\"] =  \"none\"\n",
    "mach_dfy.loc[mach_dfy[\"证据等级\"]==\"7：免疫学证据\", \"证据等级\"] =  \"3：血及无菌体液（如脓胸液）\"\n",
    "mach_dfy.loc[mach_dfy[\"证据等级\"]==\"4：气管镜活检组织培养出病原体,或者支气管肺泡灌洗液（BALF）定量培养>10**4\", \"证据等级\"] =  \"4：气管镜活检组织培养出病原体，或者支气管肺泡灌洗液（BALF）定量培养>10**4\"\n",
    "mach_dfy.loc[mach_dfy[\"是否是致病菌\"]=='yes', ['物种名称', '是否是致病菌', '样本量', '年龄', \n",
    "           '性别',  '免疫状态', '证据等级',  '原文链接', '标注方式', \"发表年份\"]].to_excel(\"output/%s/final_label.xlsx\" % file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_l = []\n",
    "for i in mach_df.index:\n",
    "    mach = mach_df.loc[i].values.tolist()\n",
    "    mach_l.append(mach)\n",
    "\n",
    "v = []\n",
    "for i in mach_l:\n",
    "    if len(i) > 3:\n",
    "        v.append(i)\n",
    "    else:\n",
    "        v.append(i[0])\n",
    "with open(\"output/%s/mach_l.json\" % file_dir, \"w\") as f:\n",
    "    b = json.dumps(v)\n",
    "    f.write(b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "214392d51fb2c1897b3bd3ad6aaa927e6326499bab7564d3ffedfb7cce17d6fb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
